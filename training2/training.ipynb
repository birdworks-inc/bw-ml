{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6680d6ad-5a4f-44c5-a554-629bc28d7a0e",
   "metadata": {},
   "source": [
    "# Amazon Sagemakerを利用したきのこの山検出AIの実装研修\n",
    "\n",
    "## 研修のゴール共有\n",
    "たけのこの里ときのこの山の何れかがベルトコンベアーで流れて来たときに、物体を認識させきのこの山を検出する。\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. モデル学習用の画像を撮影する\n",
    "    * S3バケットの確認\n",
    "    * 写真撮影及び画像のアップロード\n",
    "2. 画像にラベリングを行い学習させるインプットを整備する\n",
    "    * S3バケットへ画像の格納\n",
    "    * ラベリングを実施\n",
    "    * ラベリング結果をモデル作成用に加工\n",
    "3. FineTuneを利用し、学習インプットにてモデルの作成を行う\n",
    "    * 学習用インプットデータを指定してモデルの作成を実施\n",
    "4. モデルを起動する\n",
    "    * 作成モデルを起動\n",
    "5. 起動したモデルにてテスト判定を行う\n",
    "    * エンドポイントを利用しテストイメージで判定させる\n",
    "6. 疑似ベルトコンベアープログラムで判定させる\n",
    "    * 完成したモデルを利用し最終テストを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495256f-cbc3-4e81-8a8b-77582afe2abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update && apt-get upgrade -y\n",
    "!apt-get install libgl1-mesa-dev -y\n",
    "!pip install boto3\n",
    "!pip install opencv-python pyheif\n",
    "\n",
    "import sagemaker, json, numpy as np, os, boto3, uuid\n",
    "from PIL import Image, ImageDraw, ImageOps, ImageColor, ImageFont\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "import pyheif\n",
    "np.random.seed(seed=1234)\n",
    "print('---------- 終 了 ----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d32cb-a039-422c-865b-6869399eca16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. モデル学習用の画像を撮影する\n",
    "### S3バケットの確認\n",
    "ユーザー毎に作成されている、S3バケットが存在している事を確認する\n",
    "\n",
    "### 写真撮影及び画像のアップロード\n",
    "スマホにて画像撮影を実施し、該当イメージをアップロードする\n",
    "* Studioのアップロードアイコンをクリックしtrain_raw_imagesフォルダへアップロードする\n",
    "* ファイルフォーマットの変換及び適正サイズへリサイズを実施する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d705d-3af9-4ab3-8a06-8ca0ec25646f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format変換及びリサイズ\n",
    "## アスペクト比を固定して、幅が指定した値になるようリサイズする。\n",
    "def scale_to_width(img, width):\n",
    "    height = round(img.height * width / img.width)\n",
    "    return img.resize((width, height))\n",
    "\n",
    "def convHeic2Png(image_path):\n",
    "    new_name = image_path.replace('HEIC', 'png')\n",
    "    heif_file = pyheif.read(image_path)\n",
    "    data = Image.frombytes(\n",
    "        heif_file.mode,\n",
    "        heif_file.size,\n",
    "        heif_file.data,\n",
    "        \"raw\",\n",
    "        heif_file.mode,\n",
    "        heif_file.stride,\n",
    "        )\n",
    "    # 画像をリサイズする\n",
    "    data_resized = scale_to_width(data, 500)\n",
    "    data_resized.save(new_name, \"PNG\")\n",
    "    print(f'output heic -> png file:{new_name}')\n",
    "    \n",
    "def convJpeg2Png(image_path):\n",
    "    new_name = image_path.replace('jpeg', 'png')\n",
    "    img = Image.open(image_path)\n",
    "    # 画像をリサイズする\n",
    "    img_resized = scale_to_width(img, 500)\n",
    "    img_resized.save(new_name, \"PNG\")\n",
    "    print(f'output jpeg -> png file:{new_name}')\n",
    "\n",
    "def convPng2Png(image_path):\n",
    "    new_name = image_path\n",
    "    img = Image.open(image_path)\n",
    "    # 画像をリサイズする\n",
    "    img_resized = scale_to_width(img, 500)\n",
    "    img_resized.save(new_name, \"PNG\")\n",
    "    print(f'output png -> png file:{new_name}')\n",
    "\n",
    "print('データ変換開始')\n",
    "lst = glob(\"./train_raw_images/*.*\")\n",
    "for l in lst:\n",
    "    img = Image.open(l)\n",
    "    if l.endswith('heic'):\n",
    "        convHeic2Png(l)\n",
    "        os.remove(l)\n",
    "    elif l.endswith('jpeg'):\n",
    "        convJpeg2Png(l)\n",
    "        os.remove(l)\n",
    "    elif l.endswith('png'):\n",
    "        convPng2Png(l)\n",
    "        os.remove(l)\n",
    "    else:\n",
    "        print('ファイルタイプエラー')\n",
    "print('データ変換終了')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee9b37-3358-467c-8cd8-99ae99f8addb",
   "metadata": {},
   "source": [
    "* パラーメーターを指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4485a-bc7c-4972-bf02-bae9eb9f7e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各自のS3バケットを指定する\n",
    "S3_BUCKET='training-user1-s3'\n",
    "# 【変更不要】imageファイルの格納場所指定\n",
    "IMAGES_PATH='training2/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e9296-8e4f-49a1-86bd-cde20af655b4",
   "metadata": {},
   "source": [
    "* S3へアップロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65bd15-b2da-4d21-9695-dae55cc52c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./train_raw_images s3://{S3_BUCKET}/{IMAGES_PATH} --exclude \".*\"  --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb934c-8c97-4a95-beef-f7d2688c3995",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.画像にラベリングを行い学習させるインプットを整備する\n",
    "* ラベリングチームの作成\n",
    "グループメンバーにてラベリングチームを作成する\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-1.png)\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-2.png)\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-3.png)\n",
    "* ラベリングジョブを作成する\n",
    "    * Ground Truth>ラベリングジョブ を選択\n",
    "    * ラベリングジョブの作成 を押下する\n",
    "    * 内容を入力し実行する\n",
    "    \n",
    "    ![](./images_for_ipynb/t2-labeling-4.png)\n",
    "    \n",
    "    * タスクのタイプを指定する\n",
    "    \n",
    "    ![](./images_for_ipynb/t2-labeling-5.png)\n",
    "    \n",
    "    * ワーカーの選択とツールの設定\n",
    "    \n",
    "    ![](./images_for_ipynb/t2-labeling-6.png)\n",
    "    \n",
    "    \n",
    "* ラベリングを実施する\n",
    "    * メールが来ているのでメールのリンクに従って、ツールを開いてラベリングを行う。\n",
    "    * ラベリングが終わったことを確認する\n",
    "* ラベリング結果をモデル作成用に加工\n",
    "次のスクリプトを実行し各自用のS3バケットへ変換した結果をアップロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096d638-9660-4c8d-821b-7350321646b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ラベリングを行ったジョブ名を指定してください\n",
    "JOB_NAME=\"team-z-job2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149685b0-978c-4a80-b22f-9c85e54bd09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_MANIFEST_PATH=''.join([IMAGES_PATH,'/',JOB_NAME,'/manifests/output/output.manifest'])\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(S3_BUCKET)\n",
    "output_manifest_object = my_bucket.Object(OUTPUT_MANIFEST_PATH)\n",
    "output_manifest_json = output_manifest_object.get()[\"Body\"].read().decode('utf-8').split()\n",
    "OUTPUT_PATH = ''.join(['training2/','annotations.json'])\n",
    "\n",
    "\n",
    "# 結果のきのこの山やたけのこの里の位置情報を格納する辞書 \n",
    "annotation_dict = {\n",
    "    'images':[],\n",
    "    'annotations':[]\n",
    "}\n",
    "\n",
    "# 画像のファイル名に使う一意なシーケンス番号\n",
    "IMAGE_ID = 0\n",
    "\n",
    "# ラベリング結果の行数分ループする\n",
    "# ラベリング結果は 1 行につき 1 画像格納される\n",
    "for manifest_line in output_manifest_json:\n",
    "    # 画像のラベリング結果の読み込み\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    # print(manifest_dict)\n",
    "    # 画像のファイル名取得(ラベリング結果に格納されている)\n",
    "    filename = manifest_dict['source-ref'].split('/')[-1]\n",
    "    annotation_list = manifest_dict[JOB_NAME]['annotations']\n",
    "    # print(annotation_list)\n",
    "    # ラベリング結果を出力用辞書に格納\n",
    "    annotation_dict['images'].append(\n",
    "        {\n",
    "            'file_name' : filename,\n",
    "            'height' : manifest_dict[JOB_NAME]['image_size'][0]['height'],\n",
    "            'width' : manifest_dict[JOB_NAME]['image_size'][0]['width'],\n",
    "            'id' : IMAGE_ID\n",
    "        }\n",
    "    )\n",
    "    for annotation in annotation_list:\n",
    "        # 座標変換\n",
    "        left = annotation['left']\n",
    "        top = annotation['top']\n",
    "        right = annotation['left'] + annotation['width']\n",
    "        bottom = annotation['top'] + annotation['height']\n",
    "        # アノテーションを編集\n",
    "        annotation_dict['annotations'].append(\n",
    "            {\n",
    "                'image_id': IMAGE_ID,\n",
    "                'bbox': [left, top, right, bottom],\n",
    "                # 文字列へ変換し格納\n",
    "                # 'category_id': manifest_dict['kinotake-user1-metadata']['class-map'][str(annotation['class_id'])]\n",
    "                'category_id': annotation['class_id']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    IMAGE_ID += 1\n",
    "    \n",
    "obj = s3.Object(S3_BUCKET, OUTPUT_PATH)\n",
    "obj.put(Body = json.dumps(annotation_dict, ensure_ascii=False)) #←変数をJSON変換し S3にPUTする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0854d1-bd3e-4990-9cb1-9d099e7fc98f",
   "metadata": {},
   "source": [
    "# 3. FineTuneを利用し、学習インプットにてモデルの作成を行う\n",
    "## 学習用インプットデータを指定してモデルの作成を実施\n",
    "* JumpStartを利用し、学習用インプットイメージで転移学習を行う\n",
    "![](./images_for_ipynb/t2-model-1.png)\n",
    "* 学習用インプットイメージで転移学習を行う\n",
    "![](./images_for_ipynb/t2-model-2.png)\n",
    "* 学習用インプットイメージで転移学習を行う\n",
    "![](./images_for_ipynb/t2-model-3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b196840-8c0a-42e0-95c6-ee7575b0baea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# エンドポイントの名前\n",
    "ENDPOINT_NAME='jumpstart-ftc-team-z-endpoint-1'\n",
    "# 推論する画像の場所\n",
    "TEST_IMAGE_FILE = 'test_raw_images/lattice.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aed80-377e-4889-9be9-8c56e9db53b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論エンドポイントにアクセスするための sagemaker-runtime クライアントの生成\n",
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# 推論対象の画像を開いて変数に格納\n",
    "with open(TEST_IMAGE_FILE, 'rb') as f:\n",
    "    img_bin = f.read()\n",
    "\n",
    "# 推論を実行\n",
    "response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "\n",
    "# 推論結果を読み込む\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "# 結果を可視化\n",
    "# テスト画像を PIL を通して numpy array として開く\n",
    "image_np = np.array(Image.open(TEST_IMAGE_FILE))\n",
    "# matplotlibで描画する\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "# 推論結果を変数に展開\n",
    "bboxes, classes, confidences = model_predictions['normalized_boxes'], model_predictions['classes'], model_predictions['scores']\n",
    "# 物体検出結果を検出した分だけループする\n",
    "for idx in range(len(bboxes)):\n",
    "    # 信頼度スコアが 0.5 以上のみ可視化する　★信頼度スコアを制限したい場合はここを変更\n",
    "    if confidences[idx]>0.5:\n",
    "        # 検出した座標（左上を(0,0),右下を(1,1)とした相対座標)を取得\n",
    "        left, bot, right, top = bboxes[idx]\n",
    "        # 相対座標を絶対座標に変換する\n",
    "        x, w = [val * image_np.shape[1] for val in [left, right - left]]\n",
    "        y, h = [val * image_np.shape[0] for val in [bot, top - bot]]\n",
    "        # 検出した物体の ID を take/kino に読み替える\n",
    "        class_name = 'kino' if int(classes[idx])==0 else 'take'\n",
    "        # take/kinoに対して矩形で描画するための色を設定する\n",
    "        color = 'blue' if class_name == 'take' else 'red'\n",
    "        # matplotlib に検出した物体に矩形を描画する\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # 左上に検出結果と信頼度スコアを描画する\n",
    "        ax.text(x, y, \"{} {:.0f}%\".format(class_name, confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a1005-83b5-4cd3-b7c1-522d798dcf4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9-2. ベルトコンベアを模した推論\n",
    "* `./test_raw_images/takenoko.jpg` にタケノコが横一列に並んでいる（１つだけキノコが混在）\n",
    "* 512x512の画像をスライドしながら切り出すことでベルトコンベアでお菓子が流れているような動画として扱う\n",
    "* 各画像に対して推論をかけ、最後に１つの動画として出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831e87f-e88c-45a4-ae89-e7b8d8343b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 画像を切り出すためのコード\n",
    "# 開始地点設定\n",
    "x ,y = 0,320\n",
    "# 切り出すサイズ設定\n",
    "CROP_SIZE=(512,512)\n",
    "# 切り出す対象の画像を PIL で開く\n",
    "img = Image.open('./test_raw_images/takenoko.jpg')\n",
    "# 切り出した画像を保存するディレクトリ\n",
    "CROP_DIR = './test_crop_images/'\n",
    "# re-run 用の削除コマンド\n",
    "!rm -rf {CROP_DIR}/*.png\n",
    "# 1pxずらしてループ\n",
    "count = 1\n",
    "max_count = img.size[0]-CROP_SIZE[0]\n",
    "for i in range(max_count):\n",
    "    # 画像の切り出し\n",
    "    crop_img = img.crop((i,y,i+CROP_SIZE[0],y+CROP_SIZE[1]))\n",
    "    # 切り出した画像を保存\n",
    "    file_name = f'{CROP_DIR}{str(i).zfill(5)}.png'\n",
    "    crop_img.save(file_name)\n",
    "    print(\"\\r\"+str(count)+\"/\"+str(max_count),end=\"\")\n",
    "    count += 1\n",
    "print('終了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e13d45-1b79-4cec-95eb-715388c77352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imgInference(img_file_path):\n",
    "    # 切り出した画像を開く\n",
    "    with open(img_file_path,'rb') as f:\n",
    "        img_bin = f.read()\n",
    "    # 推論エンドポイントに画像を投げる\n",
    "    response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "    # 推論結果を読み込む\n",
    "    pred=json.loads(response['Body'].read())\n",
    "    # print(pred)\n",
    "    # 推論結果を展開\n",
    "    bboxes, classes, confidences = pred['normalized_boxes'], pred['classes'], pred['scores']\n",
    "    # print(confidences)\n",
    "    # 切り出した画像を PIL で開く\n",
    "    img = Image.open(img_file_path)\n",
    "    # 矩形やテキストを描くために draw インスタンスを生成\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # 検出したkino/take分ループ\n",
    "    for i in range(len(bboxes)):\n",
    "        # 信頼度スコアが0.8以上のみ描画する\n",
    "        if confidences[i]>0.7:\n",
    "            # 矩形の相対座標を取得\n",
    "            left, top, right, bottom = bboxes[i]\n",
    "            # 矩形の相対座標を絶対座標に変換\n",
    "            left = img.size[0] * left\n",
    "            top = img.size[1] * top\n",
    "            right = img.size[0] * right\n",
    "            bottom = img.size[1] * bottom\n",
    "            # 検出した物体の ID を take/kino に読み替える\n",
    "            text = 'take' if int(classes[i])==1 else 'kino'\n",
    "            # take/kinoに対して矩形で描画するための色を設定する\n",
    "            color = 'blue' if text == 'take' else 'red'\n",
    "            # 矩形の左上に表示する文字の大きさを設定、きのこの山なら大きくする\n",
    "            TEXTSIZE=14 if classes[i]=='1' else 18\n",
    "            # 矩形の先の太さを設定、きのこの山なら太くする\n",
    "            LINEWIDTH=4 if classes[i]=='1' else 6\n",
    "            # 矩形を描画する\n",
    "            draw.rectangle([(left,top),(right,bottom)], outline=color, width=LINEWIDTH)\n",
    "            # 矩形の左上に描画する信頼度スコアの取得\n",
    "            text += f' {str(round(confidences[i],3))}'\n",
    "            # テキストを描画する場所を取得\n",
    "            txpos = (left, top-TEXTSIZE-LINEWIDTH//2)\n",
    "            # フォントの設定\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf\", size=TEXTSIZE)\n",
    "            # 描画するテキストのサイズを取得\n",
    "            bbox = draw.textbbox(txpos, text, font=font)\n",
    "            # テキストの背景用の矩形を描画\n",
    "            draw.rectangle(bbox, outline=color, fill=color, width=LINEWIDTH)\n",
    "            # テキストを描画\n",
    "            draw.text(txpos, text, fill='white',font=font)\n",
    "    return img\n",
    "\n",
    "\n",
    "# テスト実行\n",
    "img_file_path = glob(f'{CROP_DIR}*.png')[0]\n",
    "ret = imgInference(img_file_path)\n",
    "image_np = np.array(ret)\n",
    "## matplotlibで描画する\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9deb73f-5298-4dac-b94e-ad9e43965daa",
   "metadata": {},
   "source": [
    "全データ推論して、矩形を描いた画像を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbee7d-1428-4f06-8e73-70a9a05e603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 検出結果を保存するディレクトリを設定\n",
    "DETECT_DIR='./test_detect_images/'\n",
    "# re-run 用の削除コマンド\n",
    "!rm -rf {DETECT_DIR}/*.png\n",
    "\n",
    "count = 1\n",
    "# 切り出した画像分だけループ\n",
    "for img_file_path in sorted(glob(f'{CROP_DIR}*.png')):\n",
    "    img = imgInference(img_file_path)\n",
    "    # 画像をファイルに書き出す\n",
    "    img.save(img_file_path.replace(CROP_DIR,DETECT_DIR))\n",
    "    print(\"\\r\"+str(count),end=\"\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a7ed6-16ab-4320-8343-82332e6284a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 書き出した画像を連結して動画にする    \n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "video = cv2.VideoWriter('./video.mp4',fourcc, 120.0, CROP_SIZE)\n",
    "for img_file_path in sorted(glob(f'{DETECT_DIR}*.png')):\n",
    "    img = cv2.imread(img_file_path)\n",
    "    video.write(img)\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01788ec-60a7-4f03-9d95-04536549d87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

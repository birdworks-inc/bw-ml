{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6680d6ad-5a4f-44c5-a554-629bc28d7a0e",
   "metadata": {},
   "source": [
    "# Amazon Sagemakerを利用したきのこの山検出AIの実装研修\n",
    "\n",
    "## 研修のゴール共有\n",
    "たけのこの里ときのこの山の何れかがベルトコンベアーで流れて来たときに、物体を認識させきのこの山を検出する。\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. モデル学習用の画像を撮影する\n",
    "    * S3バケットの確認\n",
    "    * 写真撮影及び画像のアップロード\n",
    "2. 画像にラベリングを行い学習させるインプットを整備する\n",
    "    * S3バケットへ画像の格納\n",
    "    * ラベリングを実施\n",
    "    * ラベリング結果をモデル作成用に加工\n",
    "3. FineTuneを利用し、学習インプットにてモデルの作成を行う\n",
    "    * 学習用インプットデータを指定してモデルの作成を実施\n",
    "4. モデルを起動する\n",
    "    * 作成モデルを起動\n",
    "5. 起動したモデルにてテスト判定を行う\n",
    "    * エンドポイントを利用しテストイメージで判定させる\n",
    "6. 疑似ベルトコンベアープログラムで判定させる\n",
    "    * 完成したモデルを利用し最終テストを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495256f-cbc3-4e81-8a8b-77582afe2abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update && apt-get upgrade -y\n",
    "!apt-get install libgl1-mesa-dev -y\n",
    "!pip install boto3\n",
    "!pip install opencv-python pyheif\n",
    "\n",
    "import sagemaker, json, numpy as np, os, boto3, uuid\n",
    "from PIL import Image, ImageDraw, ImageOps, ImageColor, ImageFont\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "import pyheif\n",
    "np.random.seed(seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d32cb-a039-422c-865b-6869399eca16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. モデル学習用の画像を撮影する\n",
    "### S3バケットの確認\n",
    "ユーザー毎に作成されている、S3バケットが存在している事を確認する\n",
    "\n",
    "### 写真撮影及び画像のアップロード\n",
    "スマホにて画像撮影を実施し、該当イメージをアップロードする\n",
    "* Studioのアップロードアイコンをクリックしtrain_raw_imagesフォルダへアップロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d705d-3af9-4ab3-8a06-8ca0ec25646f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format変換(HEIC -> PNG) & リサイズ 注）元データは削除します\n",
    "\n",
    "## アスペクト比を固定して、幅が指定した値になるようリサイズする。\n",
    "def scale_to_width(img, width):\n",
    "    height = round(img.height * width / img.width)\n",
    "    return img.resize((width, height))\n",
    "\n",
    "def conv(image_path):\n",
    "    new_name = image_path.replace('HEIC', 'png')\n",
    "    # heif_file = pillow_heif.read_heif(image_path)\n",
    "    heif_file = pyheif.read(image_path)\n",
    "    data = Image.frombytes(\n",
    "        heif_file.mode,\n",
    "        heif_file.size,\n",
    "        heif_file.data,\n",
    "        \"raw\",\n",
    "        heif_file.mode,\n",
    "        heif_file.stride,\n",
    "        )\n",
    "    # 画像をリサイズする\n",
    "    data_resized = scale_to_width(data, 500)\n",
    "    data_resized.save(new_name, \"PNG\")\n",
    "    print(f'output file:{new_name}')\n",
    "\n",
    "print('データ変換開始')\n",
    "lst = glob(\"./train_raw_images/*.HEIC\")\n",
    "for l in lst:\n",
    "    conv(l)\n",
    "print('データ変換終了')\n",
    "!rm ./train_raw_images/*.HEIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee9b37-3358-467c-8cd8-99ae99f8addb",
   "metadata": {},
   "source": [
    "* S3へアップロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4485a-bc7c-4972-bf02-bae9eb9f7e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_BUCKET='training-user1-s3'\n",
    "IMAGES_PATH='training2/labeling-images'\n",
    "!aws s3 cp ./train_raw_images s3://{S3_BUCKET}/{IMAGES_PATH} --exclude \".*\"  --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb934c-8c97-4a95-beef-f7d2688c3995",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.画像にラベリングを行い学習させるインプットを整備する\n",
    "* ラベリングチームの作成\n",
    "グループメンバーにてラベリングチームを作成する\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-1.png)\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-2.png)\n",
    "*\n",
    "![](./images_for_ipynb/t2-labeling-3.png)\n",
    "* ラベリングジョブを作成する\n",
    "* ラベリングを実施する\n",
    "    * メールが来ているのでメールのリンクに従って、ツールを開いてラベリングを行う。\n",
    "    * ラベリングが終わったことを確認する\n",
    "* ラベリング結果をモデル作成用に加工\n",
    "次のスクリプトを実行し各自用のS3バケットへ変換した結果をアップロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096d638-9660-4c8d-821b-7350321646b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"training-user1-s3\"\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149685b0-978c-4a80-b22f-9c85e54bd09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_MANIFEST_PATH = \"kinotake/images/kinotake-user1/manifests/output/output.manifest\"\n",
    "output_manifest_object = my_bucket.Object(OUTPUT_MANIFEST_PATH)\n",
    "output_manifest_json = output_manifest_object.get()[\"Body\"].read().decode('utf-8').split()\n",
    "\n",
    "\n",
    "OUTPUT_PATH = \"kinotake/annotations.json\"\n",
    "\n",
    "\n",
    "# 結果のきのこの山やたけのこの里の位置情報を格納する辞書 \n",
    "annotation_dict = {\n",
    "    'images':[],\n",
    "    'annotations':[]\n",
    "}\n",
    "\n",
    "# 画像のファイル名に使う一意なシーケンス番号\n",
    "IMAGE_ID = 0\n",
    "\n",
    "# ラベリング結果の行数分ループする\n",
    "# ラベリング結果は 1 行につき 1 画像格納される\n",
    "for manifest_line in output_manifest_json:\n",
    "    # 画像のラベリング結果の読み込み\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    # print(manifest_dict)\n",
    "    # 画像のファイル名取得(ラベリング結果に格納されている)\n",
    "    filename = manifest_dict['source-ref'].split('/')[-1]\n",
    "    annotation_list = manifest_dict['kinotake-user1']['annotations']\n",
    "    # print(annotation_list)\n",
    "    # ラベリング結果を出力用辞書に格納\n",
    "    annotation_dict['images'].append(\n",
    "        {\n",
    "            'file_name' : filename,\n",
    "            'height' : manifest_dict['kinotake-user1']['image_size'][0]['height'],\n",
    "            'width' : manifest_dict['kinotake-user1']['image_size'][0]['width'],\n",
    "            'id' : IMAGE_ID\n",
    "        }\n",
    "    )\n",
    "    for annotation in annotation_list:\n",
    "        # 座標変換\n",
    "        left = annotation['left']\n",
    "        top = annotation['top']\n",
    "        right = annotation['left'] + annotation['width']\n",
    "        bottom = annotation['top'] + annotation['height']\n",
    "        # アノテーションを編集\n",
    "        annotation_dict['annotations'].append(\n",
    "            {\n",
    "                'image_id': IMAGE_ID,\n",
    "                'bbox': [left, top, right, bottom],\n",
    "                # 文字列へ変換し格納\n",
    "                # 'category_id': manifest_dict['kinotake-user1-metadata']['class-map'][str(annotation['class_id'])]\n",
    "                'category_id': annotation['class_id']\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    IMAGE_ID += 1\n",
    "    \n",
    "# print(json.dumps(annotation_dict))\n",
    "\n",
    "# # ランダムクロップ補正後のラベリング結果を出力\n",
    "# with open('annotations.json','wt') as f:\n",
    "#     f.write(json.dumps(annotation_dict))  \n",
    "obj = s3.Object(BUCKET_NAME, OUTPUT_PATH)\n",
    "obj.put(Body = json.dumps(annotation_dict, ensure_ascii=False)) #←変数をJSON変換し S3にPUTする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b196840-8c0a-42e0-95c6-ee7575b0baea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論エンドポイントにアクセスするための sagemaker-runtime クライアントの生成\n",
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "# エンドポイントの名前\n",
    "ENDPOINT_NAME='jumpstart-ftc-user1-kinotake-endpoint'\n",
    "# 推論する画像の場所\n",
    "TEST_IMAGE_FILE = 'test_raw_images/lattice.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aed80-377e-4889-9be9-8c56e9db53b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論対象の画像を開いて変数に格納\n",
    "with open(TEST_IMAGE_FILE, 'rb') as f:\n",
    "    img_bin = f.read()\n",
    "\n",
    "# 推論を実行\n",
    "response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "\n",
    "# 推論結果を読み込む\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "# 結果を可視化\n",
    "# テスト画像を PIL を通して numpy array として開く\n",
    "image_np = np.array(Image.open(TEST_IMAGE_FILE))\n",
    "# matplotlibで描画する\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "# 推論結果を変数に展開\n",
    "bboxes, classes, confidences = model_predictions['normalized_boxes'], model_predictions['classes'], model_predictions['scores']\n",
    "# 物体検出結果を検出した分だけループする\n",
    "for idx in range(len(bboxes)):\n",
    "    # 信頼度スコアが 0.5 以上のみ可視化する\n",
    "    if confidences[idx]>0.5:\n",
    "        # 検出した座標（左上を(0,0),右下を(1,1)とした相対座標)を取得\n",
    "        left, bot, right, top = bboxes[idx]\n",
    "        # 相対座標を絶対座標に変換する\n",
    "        x, w = [val * image_np.shape[1] for val in [left, right - left]]\n",
    "        y, h = [val * image_np.shape[0] for val in [bot, top - bot]]\n",
    "        # 検出した物体の ID を take/kino に読み替える\n",
    "        class_name = 'kino' if int(classes[idx])==0 else 'take'\n",
    "        # take/kinoに対して矩形で描画するための色を設定する\n",
    "        color = 'blue' if class_name == 'take' else 'red'\n",
    "        # matplotlib に検出した物体に矩形を描画する\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # 左上に検出結果と信頼度スコアを描画する\n",
    "        ax.text(x, y, \"{} {:.0f}%\".format(class_name, confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aecff7-c3eb-4124-b972-20258986d2f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for manifest_line in output_manifest_json:\n",
    "    # 画像のラベリング結果の読み込み\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    print(manifest_dict['source-ref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0581ce-a378-40c4-8b54-fccc0bafca56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 からマニフェストファイルをLocal（manifestフォルダ）へダウンロード\n",
    "LOCAL_MANIFEST_DIR = \"./training-manifest/\"\n",
    "BASE_PREFIX = \"kinotake/images\"\n",
    "GT_JOB_NAME = \"kinotake-user1\"\n",
    "sagemaker.session.Session().download_data(LOCAL_MANIFEST_DIR, key_prefix=f'{BASE_PREFIX}/{GT_JOB_NAME}/manifests/output/output.manifest',bucket=BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8fcc0-8380-4496-a915-f19888fc039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 からLocalへラベリング済みデータのダウンロード\n",
    "TRAIN_RAWIMAGE_DIR = './training-train_raw_images/'\n",
    "import shutil\n",
    "shutil.rmtree(TRAIN_RAWIMAGE_DIR)\n",
    "os.mkdir(TRAIN_RAWIMAGE_DIR)\n",
    "\n",
    "# ラベリング結果をテキストとして読み込む\n",
    "with open(f'{LOCAL_MANIFEST_DIR}/output.manifest','r') as f:\n",
    "    manifest_line_list = f.readlines()\n",
    "\n",
    "for manifest_line in manifest_line_list:\n",
    "    # 画像のラベリング結果の読み込み\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    filename = manifest_dict['source-ref'].split('/')[-1]\n",
    "    print(filename)\n",
    "    sagemaker.session.Session().download_data(TRAIN_RAWIMAGE_DIR, key_prefix=f'{BASE_PREFIX}/{filename}',bucket=BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db34516-ebb4-4e83-8036-335401aada0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c758c1-250c-46a1-975b-adce0eef43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロップした画像にきのこの山やたけのこの里が映っている場合、\n",
    "# クロップした後のきのこの山やたけのこの里が1/4以下かどうかを判定するヘルパー関数\n",
    "\n",
    "def fix_bbox(l,t,r,b,w,h):\n",
    "    # 判定結果、NG なら False にする\n",
    "    judge = True\n",
    "    # ラベリング結果のクロップ補正後の値が負の値ならば 0 に、イメージサイズより大きければイメージサイズに補正する\n",
    "    fix_left = 0 if l < 0 else l\n",
    "    fix_top = 0 if t < 0 else t\n",
    "    fix_right = w if r > w else r\n",
    "    fix_bottom = h if b > h else b\n",
    "    # 領域外ならラベリング無しとする\n",
    "    if l > w or t > h or r < 0 or b <0:\n",
    "        judge=False\n",
    "    # 基の面積の1/4以下ならアノテーション無しとする\n",
    "    elif (r-l)*(b-t)/4 > (fix_right-fix_left)*(fix_bottom-fix_top):\n",
    "        judge=False\n",
    "    \n",
    "    return judge,(fix_left,fix_top,fix_right,fix_bottom)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcf157-f80c-496e-a440-1ef418b27139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OUTPUT_MANIFEST_PATH = \"kinotake/images/kinotake-user1/manifests/output/output.manifest\"\n",
    "# output_manifest_object = my_bucket.Object(OUTPUT_MANIFEST_PATH)\n",
    "# output_manifest_json = output_manifest_object.get()[\"Body\"].read().decode('utf-8').split()\n",
    "\n",
    "\n",
    "OUTPUT_PATH = \"kinotake/annotations.json\"\n",
    "OUTPUT_DIR = './training-train_random_crop_images/'\n",
    "shutil.rmtree(OUTPUT_DIR)\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# ラベリング結果をテキストとして読み込む\n",
    "with open('manifest/output.manifest','r') as f:\n",
    "    manifest_line_list = f.readlines()\n",
    "\n",
    "# クロップサイズの定数\n",
    "IMAGE_SIZE_TUPLE=(512,512)\n",
    "\n",
    "# 結果のきのこの山やたけのこの里の位置情報を格納する辞書 \n",
    "annotation_dict = {\n",
    "    'images':[],\n",
    "    'annotations':[]\n",
    "}\n",
    "\n",
    "# 画像のファイル名に使う一意なシーケンス番号\n",
    "IMAGE_ID = 0\n",
    "\n",
    "# ラベリング結果の行数分ループする\n",
    "# ラベリング結果は 1 行につき 1 画像格納される\n",
    "for manifest_line in manifest_line_list:\n",
    "    # 画像のラベリング結果の読み込み\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    # print(manifest_dict)\n",
    "    # 画像のファイル名取得(ラベリング結果に格納されている)\n",
    "    filename = manifest_dict['source-ref'].split('/')[-1]\n",
    "    annotation_list = manifest_dict['kinotake-user1']['annotations']\n",
    "    # 元画像のサイズを取得(ラベリング結果に格納されている)\n",
    "    image_size_tuple=(manifest_dict[GT_JOB_NAME]['image_size'][0]['width'],manifest_dict[GT_JOB_NAME]['image_size'][0]['height'])\n",
    "    # PIL で画像を開く\n",
    "    raw_img = Image.open(os.path.join(TRAIN_RAWIMAGE_DIR,filename))\n",
    "    # 20 回クロップする\n",
    "    for i in range(20):\n",
    "        # ループするかどうかのフラグ(画像にきのこの山やたけのこの里が 2 枚未満だったらクロップをやりなおし)\n",
    "        loop = True\n",
    "        while loop:\n",
    "            # クロップを行う左上の座標を設定\n",
    "            rand_x = np.random.randint(0,image_size_tuple[0]-IMAGE_SIZE_TUPLE[0])\n",
    "            rand_y = np.random.randint(0,image_size_tuple[1]-IMAGE_SIZE_TUPLE[1])\n",
    "            # クロップする\n",
    "            crop_img = raw_img.crop((\n",
    "                rand_x,\n",
    "                rand_y,\n",
    "                rand_x + IMAGE_SIZE_TUPLE[0],\n",
    "                rand_y + IMAGE_SIZE_TUPLE[1]\n",
    "            ))\n",
    "            # クロップ後のきのこの山やたけのこの里の位置を格納するリスト\n",
    "            annotation_list = []\n",
    "            # 元画像のラベリング結果をループ\n",
    "            for annotation in manifest_dict[GT_JOB_NAME]['annotations']:\n",
    "                # クロップした後のきのこの山やたけのこの里の座標に補正\n",
    "                left = annotation['left'] - rand_x\n",
    "                top = annotation['top'] - rand_y\n",
    "                right = annotation['left'] + annotation['width'] - rand_x\n",
    "                bottom = annotation['top'] + annotation['height'] - rand_y\n",
    "                # きのこの山やたけのこの里があるかどうかを判定\n",
    "                judge,(left,top,right,bottom) = fix_bbox(left,top,right,bottom,IMAGE_SIZE_TUPLE[0],IMAGE_SIZE_TUPLE[1])\n",
    "                if judge:\n",
    "                    # きのこの山やたけのこの里があったら位置とラベルを追加\n",
    "                    annotation_list.append(\n",
    "                        {\n",
    "                            'bbox':[left,top,right,bottom],\n",
    "                            'category_id':annotation['class_id']\n",
    "                        }\n",
    "                    )\n",
    "            # きのこの山やたけのこの里と数が2未満だったらクロップやり直し\n",
    "            if len(annotation_list) > 0:\n",
    "                loop = False\n",
    "        \n",
    "        # クロップしたら画像を保存する\n",
    "        save_file_name = f'{str(IMAGE_ID).zfill(5)}_{str(i).zfill(5)}_{filename}'.replace('jpg','png')\n",
    "        crop_img.save(os.path.join(OUTPUT_DIR,save_file_name))\n",
    "        \n",
    "        # 補正済ラベリング結果を出力用辞書に格納\n",
    "        annotation_dict['images'].append(\n",
    "            {\n",
    "                'file_name' : save_file_name,\n",
    "                'height' : IMAGE_SIZE_TUPLE[1],\n",
    "                'width' : IMAGE_SIZE_TUPLE[0],\n",
    "                'id' : IMAGE_ID\n",
    "            }\n",
    "        )\n",
    "        for annotation in annotation_list:                  \n",
    "            annotation_dict['annotations'].append(\n",
    "                {\n",
    "                    'image_id': IMAGE_ID,\n",
    "                    'bbox':annotation['bbox'],\n",
    "                    'category_id':annotation['category_id']\n",
    "                }\n",
    "            )\n",
    "        IMAGE_ID += 1\n",
    "    # # print(annotation_list)\n",
    "    # # ラベリング結果を出力用辞書に格納\n",
    "    # annotation_dict['images'].append(\n",
    "    #     {\n",
    "    #         'file_name' : filename,\n",
    "    #         'height' : manifest_dict['kinotake-user1']['image_size'][0]['height'],\n",
    "    #         'width' : manifest_dict['kinotake-user1']['image_size'][0]['width'],\n",
    "    #         'id' : IMAGE_ID\n",
    "    #     }\n",
    "    # )\n",
    "    # for annotation in annotation_list:\n",
    "    #     # 座標変換\n",
    "    #     left = annotation['left']\n",
    "    #     top = annotation['top']\n",
    "    #     right = annotation['left'] + annotation['width']\n",
    "    #     bottom = annotation['top'] + annotation['height']\n",
    "    #     # アノテーションを編集\n",
    "    #     annotation_dict['annotations'].append(\n",
    "    #         {\n",
    "    #             'image_id': IMAGE_ID,\n",
    "    #             'bbox': [left, top, right, bottom],\n",
    "    #             # 文字列へ変換し格納\n",
    "    #             # 'category_id': manifest_dict['kinotake-user1-metadata']['class-map'][str(annotation['class_id'])]\n",
    "    #             'category_id': annotation['class_id']\n",
    "    #         }\n",
    "    #     )\n",
    "    \n",
    "    # IMAGE_ID += 1\n",
    "    \n",
    "# ランダムクロップ補正後のラベリング結果を出力\n",
    "with open('annotations.json','wt') as f:\n",
    "    f.write(json.dumps(annotation_dict))        \n",
    "\n",
    "\n",
    "# 出力したディレクトリを prefix として使う\n",
    "prefix = OUTPUT_DIR[2:-1]\n",
    "\n",
    "# re-run 用の削除コマンド\n",
    "!aws s3 rm s3://{BUCKET_NAME}/{prefix} --recursive\n",
    "# ランダムクロップした画像をアップロード    \n",
    "image_s3_uri = sagemaker.session.Session().upload_data(OUTPUT_DIR,key_prefix=f'{prefix}/images')\n",
    "# ラベリング結果をアップロード\n",
    "annotatione_s3_uri = sagemaker.session.Session().upload_data('./annotations.json',key_prefix=prefix)\n",
    "# Fine-Tune で使う URI を出力\n",
    "paste_str = image_s3_uri.replace('/images','')\n",
    "print(f\"paste string to S3 bucket address：{paste_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57efa7a-1d16-403d-8daf-fb355b27f353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 入力画像確認\n",
    "FILE_NAME = '00082_00002_32928BBF-99B8-4F05-ACB6-2599C9554A63_1_105_c.jpeg'\n",
    "INPUT_IMAGE_FILE = f'{OUTPUT_DIR}{FILE_NAME}'\n",
    "print(INPUT_IMAGE_FILE)\n",
    "# 推論対象の画像を開いて変数に格納\n",
    "with open(INPUT_IMAGE_FILE, 'rb') as f:\n",
    "    img_bin = f.read()\n",
    "\n",
    "# ラベリング結果(json)を読み込む\n",
    "with open('annotations.json') as f:\n",
    "    annotations_json = json.load(f)\n",
    "\n",
    "image_id = 0\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "# 対象イメージを特定\n",
    "for annotations_image in annotations_json['images']:\n",
    "    if annotations_image['file_name'] == FILE_NAME:\n",
    "        print(annotations_image)\n",
    "        image_id = annotations_image['id']\n",
    "\n",
    "image_np = np.array(Image.open(INPUT_IMAGE_FILE))\n",
    "# matplotlibで描画する\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "# イメージに対するアノテーションを取得\n",
    "for annotations_row in annotations_json['annotations']:\n",
    "    if annotations_row['image_id'] == image_id:\n",
    "        # 検出した座標（左上を(0,0),右下を(1,1)とした相対座標)を取得\n",
    "        left, top, right, bot = annotations_row['bbox']\n",
    "         # 相対座標を絶対座標に変換する\n",
    "        x = left\n",
    "        w = right - left\n",
    "        y = bot\n",
    "        h = top - bot\n",
    "         # 検出した物体の ID を take/kino に読み替える\n",
    "        class_name = 'kino' if int(annotations_row['category_id'])==0 else 'take'\n",
    "         # take/kinoに対して矩形で描画するための色を設定する\n",
    "        color = 'blue' if class_name == 'take' else 'red'\n",
    "         # matplotlib に検出した物体に矩形を描画する\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # 左上に検出結果と信頼度スコアを描画する\n",
    "        # ax.text(x, y, \"{} {:.0f}%\".format(class_name, 0), bbox=dict(facecolor='white', alpha=0.5))\n",
    "        ax.text(x, y, \"{}\".format(class_name), bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a1005-83b5-4cd3-b7c1-522d798dcf4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9-2. ベルトコンベアを模した推論\n",
    "* `./traning-test_raw_images/takenoko.jpg` にタケノコが横一列に並んでいる（１つだけキノコが混在）\n",
    "* 512x512の画像をスライドしながら切り出すことでベルトコンベアでお菓子が流れているような動画として扱う\n",
    "* 各画像に対して推論をかけ、最後に１つの動画として出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831e87f-e88c-45a4-ae89-e7b8d8343b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 画像を切り出すためのコード\n",
    "# 開始地点設定\n",
    "x ,y = 0,512\n",
    "# 切り出すサイズ設定\n",
    "CROP_SIZE=(512,512)\n",
    "# 切り出す対象の画像を PIL で開く\n",
    "img = Image.open('./traning-test_raw_images/takenoko.jpg')\n",
    "# 切り出した画像を保存するディレクトリ\n",
    "CROP_DIR = './traning-test_crop_images/'\n",
    "# re-run 用の削除コマンド\n",
    "!rm -rf {CROP_DIR}/*.png\n",
    "# 1pxずらしてループ\n",
    "for i in range(img.size[0]-CROP_SIZE[0]):\n",
    "    # 画像の切り出し\n",
    "    crop_img = img.crop((i,y,i+CROP_SIZE[0],y+CROP_SIZE[1]))\n",
    "    # 切り出した画像を保存\n",
    "    file_name = f'{CROP_DIR}{str(i).zfill(5)}.png'\n",
    "    crop_img.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9deb73f-5298-4dac-b94e-ad9e43965daa",
   "metadata": {},
   "source": [
    "全データ推論して、矩形を描いた画像を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fbee7d-1428-4f06-8e73-70a9a05e603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 検出結果を保存するディレクトリを設定\n",
    "DETECT_DIR='./traning-test_detect_images/'\n",
    "# re-run 用の削除コマンド\n",
    "!rm -rf {DETECT_DIR}/*.png\n",
    "\n",
    "# 切り出した画像分だけループ\n",
    "for img_file_path in sorted(glob(f'{CROP_DIR}*.png')):\n",
    "    # 切り出した画像を開く\n",
    "    with open(img_file_path,'rb') as f:\n",
    "        img_bin = f.read()\n",
    "    # 推論エンドポイントに画像を投げる\n",
    "    response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "    # 推論結果を読み込む\n",
    "    pred=json.loads(response['Body'].read())\n",
    "    # 推論結果を展開\n",
    "    bboxes, classes, confidences = pred['normalized_boxes'], pred['classes'], pred['scores']\n",
    "    # 切り出した画像を PIL で開く\n",
    "    img = Image.open(img_file_path)\n",
    "    # 矩形やテキストを描くために draw インスタンスを生成\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # 検出したkino/take分ループ\n",
    "    for i in range(len(bboxes)):\n",
    "        # 信頼度スコアが0.8以上のみ描画する\n",
    "        if confidences[i]>0.8:\n",
    "            # 矩形の相対座標を取得\n",
    "            left, top, right, bottom = bboxes[i]\n",
    "            # 矩形の相対座標を絶対座標に変換\n",
    "            left = img.size[0] * left\n",
    "            top = img.size[1] * top\n",
    "            right = img.size[0] * right\n",
    "            bottom = img.size[1] * bottom\n",
    "            # 検出した物体の ID を take/kino に読み替える\n",
    "            text = 'take' if int(classes[i])==1 else 'kino'\n",
    "            # take/kinoに対して矩形で描画するための色を設定する\n",
    "            color = 'blue' if text == 'take' else 'red'\n",
    "            # 矩形の左上に表示する文字の大きさを設定、きのこの山なら大きくする\n",
    "            TEXTSIZE=14 if classes[i]=='1' else 18\n",
    "            # 矩形の先の太さを設定、きのこの山なら太くする\n",
    "            LINEWIDTH=4 if classes[i]=='1' else 6\n",
    "            # 矩形を描画する\n",
    "            draw.rectangle([(left,top),(right,bottom)], outline=color, width=LINEWIDTH)\n",
    "            # 矩形の左上に描画する信頼度スコアの取得\n",
    "            text += f' {str(round(confidences[i],3))}'\n",
    "            # テキストを描画する場所を取得\n",
    "            txpos = (left, top-TEXTSIZE-LINEWIDTH//2)\n",
    "            # フォントの設定\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf\", size=TEXTSIZE)\n",
    "            # 描画するテキストのサイズを取得\n",
    "            txw, txh = draw.textsize(text, font=font)\n",
    "            # テキストの背景用の矩形を描画\n",
    "            draw.rectangle([txpos, (left+txw, top)], outline=color, fill=color, width=LINEWIDTH)\n",
    "            # テキストを描画\n",
    "            draw.text(txpos, text, fill='white',font=font)\n",
    "    # 画像をファイルに書き出す\n",
    "    img.save(img_file_path.replace(CROP_DIR,DETECT_DIR))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
